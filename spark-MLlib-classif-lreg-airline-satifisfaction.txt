---- Feature extraction & Data Munging --------------

val rdd = sc.textFile("airline/airline_satisfaction_train.csv").map( x => x.split(",")).map( x => {
   val arr_size = x.size
   x.slice(2,arr_size)
 })

val categories = rdd.map( x => x(22)).distinct.zipWithIndex.collect.toMap
categories: scala.collection.immutable.Map[String,Long] = Map(neutral or dissatisfied -> 0, satisfied -> 1)

val categ_gender = rdd.map(x => x(0)).distinct.zipWithIndex.collect.toMap
categ_gender: scala.collection.immutable.Map[String,Long] = Map(Male -> 0, Female -> 1)

val categ_ctype = rdd.map(x => x(1)).distinct.zipWithIndex.collect.toMap
categ_ctype: scala.collection.immutable.Map[String,Long] = Map(Loyal Customer -> 0, disloyal Customer -> 1)

val categ_trtype = rdd.map(x => x(3)).distinct.zipWithIndex.collect.toMap
categ_trtype: scala.collection.immutable.Map[String,Long] = Map(Personal Travel -> 0, Business travel -> 1)

---- Conversion to 1-to-k binary encoding vectors 

def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collect.toMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
      val categoryIdx = categories(r(idx)).toInt
      val categoryFeatures = Array.ofDim[Double](numCategories)
      categoryFeatures(categoryIdx) = 1.0
      categoryFeatures
  })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd,4)

val rdd1 = rdd.map( x => {
  val y = Array(categories(x(22)),categ_gender(x(0)),categ_ctype(x(1)),x(2),categ_trtype(x(3))) ++ x.slice(5,21)
  y.map( z => z.toString.toDouble)
})

val vect = rdd1.zip(concat).map(x => (x._1.toList ++ x._2.toList).toArray)

val categ_class = rdd.map(x => x(4)).distinct.zipWithIndex.collect.toMap
categ_class: scala.collection.immutable.Map[String,Long] = Map(Eco Plus -> 0, Business -> 1, Eco -> 2)

val rdd1_dt = rdd.map( x => {
  val y = Array(categories(x(22)),categ_gender(x(0)),categ_ctype(x(1)),x(2),categ_trtype(x(3)),categ_class(x(4))) ++ x.slice(5,21)
  y.map( z => z.toString.toDouble)
})

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.zip(rdd1_dt).map( x => {
   val x1 = x._1
   val l1 = x1(0)
   val f1 = x1.slice(1,x1.size)
   
   val x2 = x._2
   val l2 = x2(0)
   val f2 = x2.slice(1,x2.size)
   
   (LabeledPoint(l1,Vectors.dense(f1)),LabeledPoint(l2,Vectors.dense(f2)))
 })
 
val sets = data.randomSplit(Array(0.8,0.2))
val trainSet = sets(0).map( x => x._1)
val testSet = sets(1).map( x => x._1)

trainSet.cache

---- MLlib logistic regression --------------

import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
val numIterations = 100
val model = LogisticRegressionWithSGD.train(trainSet, numIterations)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res28: Array[(Double, Double)] = Array((1.0,0.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,0.0), (1.0,0.0), (1.0,1.0), (1.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 9123
validPredicts.count                            // 20842
model.getClass.getSimpleName
metrics.areaUnderPR   // 0.4367358740760815
metrics.areaUnderROC  // 0.5013092777561226

---- MLlib SVM regression --------------

import org.apache.spark.mllib.classification.SVMWithSGD
val numIterations = 100
val model = SVMWithSGD.train(trainSet, numIterations)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res34: Array[(Double, Double)] = Array((1.0,0.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,0.0), (1.0,1.0), (1.0,0.0), (1.0,0.0), (1.0,0.0), (1.0,1.0), (1.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 9247
validPredicts.count                            // 20842
model.getClass.getSimpleName
metrics.areaUnderPR   // 0.43852231879587567
metrics.areaUnderROC  // 0.5048762431040493

---- MLlib Maive Bayes regression --------------

import org.apache.spark.mllib.classification.NaiveBayes
val model = NaiveBayes.train(trainSet)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

validPredicts.take(20)
res40: Array[(Double, Double)] = Array((0.0,0.0), (1.0,0.0), (1.0,1.0), (0.0,1.0), (1.0,0.0), (1.0,0.0), (0.0,1.0), (1.0,0.0), (0.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (1.0,0.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 12833
validPredicts.count                            // 20842
model.getClass.getSimpleName
metrics.areaUnderPR   //  0.5328215646166563
metrics.areaUnderROC  //  0.6073815767626185

----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

----- with MLlib logistic regression ----------------------

import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
val numIterations = 100
val model = LogisticRegressionWithSGD.train(trainScaled, numIterations)

val validPredicts = testSet.map(x => (model.predict(scaler.transform(x.features)),x.label))

validPredicts.take(20)
res46: Array[(Double, Double)] = Array((0.0,0.0), (0.0,0.0), (1.0,1.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (1.0,1.0), (1.0,1.0), (1.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 17848
validPredicts.count                            // 20842
model.getClass.getSimpleName
metrics.areaUnderPR    // 0.7911791780272617
metrics.areaUnderROC   // 0.8572163057330613

----- with MLlib SVM regression ----------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

import org.apache.spark.mllib.classification.SVMWithSGD
val numIterations = 100
val model = SVMWithSGD.train(trainScaled, numIterations)

val validPredicts = testSet.map(x => (model.predict(scaler.transform(x.features)),x.label))

validPredicts.take(20)
res52: Array[(Double, Double)] = Array((0.0,0.0), (0.0,0.0), (1.0,1.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (1.0,1.0), (1.0,1.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 17968
validPredicts.count                            // 20842
model.getClass.getSimpleName
metrics.areaUnderPR    // 0.798692913518858
metrics.areaUnderROC   // 0.8629448464276197

----- with MLlib Decision tree regression ----------------------

val trainSet = sets(0).map( x => x._2)
val testSet = sets(1).map( x => x._2)

trainSet.cache

import org.apache.spark.mllib.tree.DecisionTree
import org.apache.spark.mllib.tree.model.DecisionTreeModel

val categoricalFeaturesInfo = Map[Int, Int]( 0->2, 1->2, 3->2, 4->3)

val model = DecisionTree.trainClassifier(trainSet, 2, categoricalFeaturesInfo, "gini", 30, 32)

scala> model.toDebugString
res59: String =
"DecisionTreeModel classifier of depth 30 with 6809 nodes
  If (feature 11 <= 3.5)
   If (feature 6 <= 0.5)
    If (feature 8 <= 1.5)
     If (feature 10 <= 0.5)
      If (feature 8 <= 0.5)
       Predict: 1.0
      Else (feature 8 > 0.5)
       Predict: 0.0
     Else (feature 10 > 0.5)
      Predict: 1.0
    Else (feature 8 > 1.5)
     Predict: 0.0
   Else (feature 6 > 0.5)
    If (feature 6 <= 3.5)
     If (feature 4 in {0.0,2.0})
      If (feature 8 <= 3.5)
       If (feature 3 in {0.0})
        Predict: 0.0
       Else (feature 3 not in {0.0})
        If (feature 1 in {1.0})
         If (feature 5 <= 966.0)
          If (feature 4 in {2.0})
           Predict: 0.0
          Else (feature 4 not in {2.0})
           If (feature 18 <= 4.5)
            Predict: 0.0
     ...

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

scala> validPredicts.take(20)
res60: Array[(Double, Double)] = Array((0.0,0.0), (0.0,0.0), (1.0,1.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,1.0), (0.0,0.0), (1.0,1.0), (1.0,1.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0), (0.0,0.0), (0.0,0.0), (1.0,1.0), (0.0,0.0))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 19671
validPredicts.count                            // 20842
model.getClass.getSimpleName
metrics.areaUnderPR    // 0.9184849752345806
metrics.areaUnderROC   // 0.9430754750167746